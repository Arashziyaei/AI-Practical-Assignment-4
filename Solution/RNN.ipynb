{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=150 height=150> <br>\n",
    "<font color=0F5298 size=7>\n",
    "Artificial Intelligence <br>\n",
    "<font color=2565AE size=5>\n",
    "Computer Engineering Department <br>\n",
    "FALL 2024<br>\n",
    "<font color=3C99D size=5>\n",
    "Practical Assignment 4 - Neural Networks <br>\n",
    "<font color=696880 size=4>\n",
    "Arash Ziyaei Razban\n",
    "\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Data\n",
    "Please fill in your details below to help us keep track of your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: `here`\n",
    "\n",
    "Student ID: `here`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "Nowadays machine learning algorithms and models are used to solve too many types of problems. One of the most important problems is sequence modeling. Sequence modeling is the ability of an algorithm to model, interpret, make predictions about, or generate sequence data like audio, text, etc.\n",
    "\n",
    "One of the most used algorithms to solve sequence modeling problems is the Recurrent Neural Network which is a specialized form of the classical Artificial Neural Network (Multi-Layer Perceptron). Below is a picture of this network architecture:\n",
    "\n",
    "<img src=\"RNN.jpg\" width=\"400\" height=\"200\">\n",
    "\n",
    "In sequences there is a common feature, that each value has some dependency on its previous members. A simple example is arithmetic sequence: $a_i = i * d + a_0, 0 \\leq i$, where $a_n$ is dependent on $a_{n-1}$. So based on this nature of the sequences, there is a loop inside the RNN cell. This helps the RNN cell to remember some information about the previous activation values. \n",
    "\n",
    "In general, RNN is used to model sequences because of the following advantages:\n",
    "+ handles variable-length dependencies.\n",
    "+ Tracks dependencies.\n",
    "+ Maintains information about the order.\n",
    "+ Shares parameters across the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What you are going to do in this notebook\n",
    "\n",
    "+ `Step1.` You have to design a simple neural network that has only 4 neurons, inspired by the RNNs architecture, and use it to predict the next number of an arithmetic sequence.\n",
    "\n",
    "+ `Step2.` You have to increase the number of the neurons to 8. Then you have create a complext dataset of arithmatic sequences, that in each with a number of `d` called $1 \\leq n$.\n",
    "\n",
    "+ `Step3.` In the previous sections, you will find out why just using the basic architecture of RNNs does not work for complex data. So, for this step, you have to implement a Gated Recurrent Unit (GRU) from scrach to train and test it over the MNIST dataset and compare it with PyTorch GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# START TO LEARN ABOUT RNNs (30 points)\n",
    "Step 1. First, we start with simple data, like arithmetic sequence: $a_i = i * d + a_0, 0 \\leq i$.\n",
    "\n",
    "You have to design a simple neural network (only 4 neurons) inspired from the RNNs to predict the next number in the sequence.\n",
    "\n",
    "`Note`: First, try to solve this problem theoretically (find the final values for your network's weights) and then implement it.\n",
    "\n",
    "`your answer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def CustomDateset(max_length = 20, size = 10000): \n",
    "    \"\"\"\n",
    "        Generate your dataset with following details:\n",
    "        -100 <= a_i <= 100\n",
    "        -20 <= d <= 20\n",
    "        len(sequence) <= max_length\n",
    "\n",
    "        Parameters:\n",
    "            - max_length = 20\n",
    "            - size = 10000\n",
    "\n",
    "        Returns:\n",
    "            - dataset = [(length, sequence, next_number in the sequence) for each sequence]\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for _ in range(size):\n",
    "        a0 = (np.random.rand() - 0.5) * 200\n",
    "        d = (np.random.rand() - 0.5) * 40\n",
    "        length = np.random.randint(2, max_length + 1)\n",
    "        sequence = np.zeros(length)\n",
    "\n",
    "        for i in range(length):\n",
    "            sequence[i] = a0 + i * d\n",
    "\n",
    "        next_number = sequence[-1] + d\n",
    "        dataset.append((length, sequence, next_number))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now create the model:\n",
    "Based on the this task design your model. \n",
    "+ Note that, your model should inspire from the behavior of RNNs.\n",
    "+ Solve this problem theoretically. Then you will find out you only need 4 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "        Design your model.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 2, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "Now you have to train your model. \n",
    "\n",
    "Note, your architecture must be based on RNNs. This means that you have to set a hidden state called `h` and use it to create a loop for your model like RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 929.6912549997407\n",
      "Epoch 2, Loss: 70.12488075126288\n",
      "Epoch 3, Loss: 49.73847825627583\n",
      "Epoch 4, Loss: 38.71680814639244\n",
      "Epoch 5, Loss: 12.30737894583434\n",
      "Epoch 6, Loss: 4.199546368913758\n",
      "Epoch 7, Loss: 0.4655703766603276\n",
      "Epoch 8, Loss: 0.06747020009204557\n",
      "Epoch 9, Loss: 0.0703279288034237\n",
      "Epoch 10, Loss: 0.06706174495638836\n"
     ]
    }
   ],
   "source": [
    "# create your dataset, using the CustomeDataset function\n",
    "CustomDateset = CustomeDateset()\n",
    "\n",
    "# Initialize model, loss function (use MSE Loss function), and optimizer (use adam optimizer)\n",
    "model = Model()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# TODO\n",
    "# Training loop\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for length, x, y in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        h = torch.zeros(1)\n",
    "        # set x = sequences\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        # set y = next numbers\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        # initialize your output\n",
    "        output = None\n",
    "        \n",
    "        # create a loop: use hidden state\n",
    "        for i in range(length):\n",
    "            model_input = torch.cat([h, x[i].unsqueeze(0)])\n",
    "            output = model(model_input)\n",
    "            h = output[0].unsqueeze(0)\n",
    "            \n",
    "        # get loss\n",
    "        loss = criterion(output[1], y)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "        # compute total loss\n",
    "        total_loss += loss.item() \n",
    "\n",
    "    # print total loss after each epoch\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights Evaluation\n",
    "Print the weights of the model.\n",
    "\n",
    "You can see, model's weight are exactly same as what you have found in theoretical way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[ 4.4537e-05, -8.6494e-01],\n",
       "                      [ 1.1557e+00,  1.9997e+00]]))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write your opinion\n",
    "How this model use hidden state? Why hidden state is helpful? \n",
    "\n",
    "`your answer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of long-term dependencies (40 points)\n",
    "Step 2. Now, we are going to do some analysis. In the previous, both the dataset and model were simple. Now, we want to make it a little complex. \n",
    "\n",
    "+ Assume the lengths for all sequences are equal.\n",
    "+ Consider each sequence has `n` number of `d`:\n",
    "    - e.g, $a_0, a_0 + d_0, a_1 + d_1, a_2 + d_2, ...., a_{n-1} + d_{n-1}, a_{n} + d_0, a_{n+1} + d_1, ...$\n",
    "+ Change your model: just increase the number of neurons to 8. Also, because your input's dimension has changed, you have to increase the hidden state `h` dimension.\n",
    "+ Create a loop over all possible $2 \\leq n \\leq 10$ and generate a dataset for each one. Then train a new model for each dataset.\n",
    "+ At the end, show a `TotalLoss - numberOfd` plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new model\n",
    "# TODO\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "        Design your model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 4, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def CustomDateset(n, length = 20, size = 10000):\n",
    "    \"\"\"\n",
    "        Generate your dataset with following details:\n",
    "        -100 <= a_i <= 100\n",
    "        -20 <= d <= 20\n",
    "        \n",
    "        Parameters:\n",
    "            - n: number of d in sequences\n",
    "            - length = 20 (length of all sequences)\n",
    "            - size = 10000\n",
    "\n",
    "        Returns:\n",
    "            - dataset = [(length, sequence, next_number in the sequence) for each sequence]\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for _ in range(size):\n",
    "        a0 = (np.random.rand() - 0.5) * 200\n",
    "        d = [(np.random.rand() - 0.5) * 40 for _ in range(n)]\n",
    "        sequence = np.zeros(length)\n",
    "        sequence[0] = a0\n",
    "        j = 0\n",
    "        for i in range(1, length):\n",
    "            sequence[i] = sequence[i-1] + d[j % n]\n",
    "            j += 1\n",
    "\n",
    "        next_number = sequence[length-1] + d[j % n]\n",
    "        dataset.append((length, sequence, next_number))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558.5305321096907\n",
      "5.349955249368436\n",
      "1.3506590926450954\n",
      "0.6965439455625275\n",
      "0.6639816734245758\n",
      "0.4329202084140118\n",
      "0.5926944617877782\n",
      "0.5007062480522309\n",
      "0.39795856616514785\n",
      "0.44036191899561355\n",
      "0.44036191899561355\n",
      "1066.2261156673949\n",
      "19.41554730490807\n",
      "8.182294676126823\n",
      "6.18769641626529\n",
      "4.326794645122564\n",
      "3.239498324048536\n",
      "2.3155119448422243\n",
      "1.1923976119569284\n",
      "1.0164809430938606\n",
      "0.6052892313738772\n",
      "0.6052892313738772\n",
      "462.9152921058273\n",
      "18.364823234282046\n",
      "15.65532530393657\n",
      "15.245374154109223\n",
      "14.91887298630279\n",
      "14.633169872876012\n",
      "14.32035990203153\n",
      "13.901519811566759\n",
      "13.301676840459578\n",
      "12.472147717696984\n",
      "12.472147717696984\n",
      "377.65390178384325\n",
      "47.00250992702839\n",
      "28.793254546155925\n",
      "28.742173226894362\n",
      "28.581330261303286\n",
      "27.990486405753327\n",
      "26.30379078512925\n",
      "22.988017024018564\n",
      "19.2577362386577\n",
      "16.56427867439143\n",
      "16.56427867439143\n",
      "432.9824906124587\n",
      "44.55033261917153\n",
      "34.50648052304584\n",
      "25.31210212565438\n",
      "23.212652870762373\n",
      "22.441775153395533\n",
      "22.064991663646342\n",
      "21.882859312697484\n",
      "21.78646245820427\n",
      "21.727882282264908\n",
      "21.727882282264908\n",
      "150.28415280115283\n",
      "79.92255751818713\n",
      "51.41648640988577\n",
      "43.55366086728787\n",
      "38.884276267782866\n",
      "35.26661618813167\n",
      "32.46482285033339\n",
      "30.41657848776818\n",
      "28.99106818420187\n",
      "28.00875519721851\n",
      "28.00875519721851\n",
      "405.4208717313675\n",
      "131.02408525268427\n",
      "110.99832399756124\n",
      "71.48203925377392\n",
      "71.04392211242575\n",
      "70.99593713007272\n",
      "70.96706323479026\n",
      "70.93886271806711\n",
      "70.91102348876603\n",
      "70.88395848388376\n",
      "70.88395848388376\n",
      "339.879857611366\n",
      "124.70092199734478\n",
      "123.10195587953034\n",
      "122.76033508201431\n",
      "122.58714792596514\n",
      "122.3824991205121\n",
      "98.11309744988033\n",
      "67.89929992647882\n",
      "63.49939332181747\n",
      "61.89689858254096\n",
      "61.89689858254096\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "losses = []\n",
    "epochs = 10\n",
    "for n in range(2, 10):\n",
    "    total_loss = 0\n",
    "    # create your dataset, using the CustomeDataset function\n",
    "\n",
    "    dataset = CustomDateset(n=n)\n",
    "\n",
    "    # Initialize model, loss function (user MSE Loss function), and optimizer (use adam optimizer)\n",
    "\n",
    "    model = Model()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "    loss = 0\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for length, x, y in dataset:\n",
    "            optimizer.zero_grad()\n",
    "            h = torch.zeros(3)\n",
    "\n",
    "            # set x = sequences\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "            # set y = next numbers\n",
    "            y = torch.tensor(y, dtype=torch.float32)\n",
    "            # initialize your output\n",
    "            output = None\n",
    "            \n",
    "            # create a loop: use hidden state\n",
    "            for i in range(length):\n",
    "                model_input = torch.cat([h, x[i].unsqueeze(0)])\n",
    "                output = model(model_input)\n",
    "                h = output[0:3]\n",
    "                \n",
    "            # get loss\n",
    "            loss = criterion(output[3], y)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            # compute total loss\n",
    "            total_loss += loss.item() \n",
    "\n",
    "        loss = total_loss / len(dataset)\n",
    "        scheduler.step(loss)\n",
    "\n",
    "    losses.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot your amount of loss and number of d in datasets\n",
    "# TODO\n",
    "plt.plot([n for n in range(2, 10)], losses, marker='o')\n",
    "plt.title(\"loss - num of d\")\n",
    "plt.xlabel('num of d')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write your opinion\n",
    "\n",
    "+ You can see, as the number of d in the sequences increases the amount of the total loss increases too. Why?\n",
    "    - `your answer`\n",
    "\n",
    "+ Can you mathematically explain your opinion? (help: There are some gradient issues!)\n",
    "    - `your answer`\n",
    "\n",
    "+ Can you explain the problem of Long-term dependencies in RNNs?\n",
    "    - `your answer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Units (GRUs) (30 points)\n",
    "\n",
    "Step3. In the previous section, you saw, simple recurrent models cannot remember information from the past. This is a big problem because in real-world applications sequences have a lot of dependencies on their past time steps and we have to find a way to remember the information from the past. To address this problem, researchers have designed other architecture that can selectively remember or forget information over time. Like: `Long-Short Term Memory (LSTM)` and `Gated Recurrent Unit (GRU)`. The key building block behind these architectures is `gate`.\n",
    "\n",
    "#### GATE:\n",
    "These networks, use gates to track information throughout many time steps:\n",
    "+ Add information, when the information is needed.\n",
    "+ Remover information, When information is not important\n",
    "\n",
    "<img src=\"GATE.jpg\" width=\"400\" height=\"200\">\n",
    "\n",
    "#### Long-Short Term Memory:\n",
    "LSTM is an improved version of RNNs. In a classical recurrent model, there is a single hidden state `h` which is used to help the model to remember some information over time. But this is not enough to learn long-term dependencies. Using gates, LSTM is capable of addressing the problem of long-term dependencies. \n",
    "\n",
    "LSTM cell:\n",
    "\n",
    "<img src=\"LSTM.jpg\" width=\"400\" height=\"200\">\n",
    "\n",
    "GATES:\n",
    "+ <font color=green size=3> Forget gate:</font> To forget irrelevant information.\n",
    "+ <font color=red size=3> Store gate:</font> Decide what part of new is relevant.\n",
    "+ <font color=orange size=3> Update:</font> update cell state values.\n",
    "+ <font color=blue size=3> Output gate:</font> Controls what information is sent to the next time step.\n",
    " \n",
    "This architecture addresses the gradient issues you have described in the previous section. (How?)\n",
    "\n",
    "`write your answer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gated Recurrent Unit (GRU):\n",
    "Like LSTM, GRU is designed to model sequential data by allowing information to be selectively remembered or forgotten over time.\n",
    "\n",
    "GRU cell:\n",
    "\n",
    "<img src=\"GRU.jpg\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GATES and other parts:\n",
    "+ <font color=green size=3> Reset gate:</font> How much of previous hidden state to forget.\n",
    "    - $r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$\n",
    "+ <font color=orange size=3> Update gate:</font> How much of the candidate activation vector to incorporate into the new hidden state.\n",
    "    - $z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$\n",
    "    \n",
    "+ <font color=red size=3> Candidate Activation vector:</font> Computed using the current input x and modified version of the previous hidden state that is `reset` by the reset gate.\n",
    "    - $\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t])$\n",
    "\n",
    "+ <font color=blue size=3> Hidden state</font>\n",
    "    - $h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$\n",
    "\n",
    "Note: the $W_r$, $W_z$, $W_h$ are the weights associated with the GRU cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Implementation\n",
    "Now, implement a Gated Recurrent Unit from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_cell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_calsses):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_calsses\n",
    "\n",
    "        self.reste_gate = nn.Linear(self.input_dim + self.hidden_dim, self.hidden_dim)\n",
    "        self.update_gate = nn.Linear(self.input_dim + self.hidden_dim, self.hidden_dim)\n",
    "        self.candidate = nn.Linear(self.input_dim + self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        self.output = nn.Linear(self.hidden_dim, self.num_classes)\n",
    "        \n",
    "        # TODO\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, h, end = False):\n",
    "        # TODO\n",
    "        if not end:\n",
    "            # reset gate\n",
    "            vec = torch.cat([x, h], dim = 1)\n",
    "            r = self.sigmoid(self.reste_gate(vec))\n",
    "            \n",
    "            # update gate\n",
    "            z = self.sigmoid(self.update_gate(vec))\n",
    "\n",
    "            # candidate\n",
    "            vec1 = torch.cat([x, torch.mul(r, h)], dim = 1)\n",
    "            h_tilde = self.tanh(self.candidate(vec1)) \n",
    "\n",
    "            # hidden state\n",
    "            h = torch.mul(z, h) + torch.mul((1 - z), h_tilde)\n",
    "            return h\n",
    "        \n",
    "        else:\n",
    "            return self.output(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST \n",
    "MNIST database is a collection of handwritten digits. You are going to use this database to train and test your GRU cell.\n",
    "\n",
    "First, to download this database run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = MNIST(root = 'data/', download = True)\n",
    "# TODO\n",
    "train_data = MNIST(root = 'data/', train = True, transform = transforms.ToTensor())\n",
    "test_data = MNIST(root = 'data/', train = False, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the image and label of one instance from the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAan0lEQVR4nO3df2xV9f3H8dflRy+o7YVa2ts7CrYgsgh0G5PaoExHQ1sXwq9t/soCi4HAihmi03RR0G1ZlW+ixIVhliyAi/hrCkwSSaDYEl3BgBLCfnS06yyktMxm3FuKLYx+vn8Q77xSwHO5t+/29vlITkLvPZ/et4eTPj3t5dTnnHMCAKCPDbEeAAAwOBEgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgYpj1AF/W09OjlpYWpaeny+fzWY8DAPDIOaeOjg6FQiENGXL565x+F6CWlhbl5eVZjwEAuEbHjx/X2LFjL/t8v/sWXHp6uvUIAIAEuNrX86QFaMOGDbrppps0YsQIFRUV6cMPP/xK6/i2GwCkhqt9PU9KgF5//XWtXr1aa9eu1UcffaTCwkKVlpbq1KlTyXg5AMBA5JJgxowZrqKiIvrxhQsXXCgUclVVVVddGw6HnSQ2NjY2tgG+hcPhK369T/gV0Llz53To0CGVlJREHxsyZIhKSkpUV1d3yf7d3d2KRCIxGwAg9SU8QJ9++qkuXLignJycmMdzcnLU2tp6yf5VVVUKBALRjXfAAcDgYP4uuMrKSoXD4eh2/Phx65EAAH0g4f8OKCsrS0OHDlVbW1vM421tbQoGg5fs7/f75ff7Ez0GAKCfS/gVUFpamqZPn67q6uroYz09PaqurlZxcXGiXw4AMEAl5U4Iq1ev1uLFi/Xtb39bM2bM0Pr169XZ2akf//jHyXg5AMAAlJQA3Xvvvfr3v/+tNWvWqLW1Vd/4xje0a9euS96YAAAYvHzOOWc9xBdFIhEFAgHrMQAA1ygcDisjI+Oyz5u/Cw4AMDgRIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATCQ/Q008/LZ/PF7NNnjw50S8DABjghiXjk956663as2fP/15kWFJeBgAwgCWlDMOGDVMwGEzGpwYApIik/Azo2LFjCoVCKigo0IMPPqjm5ubL7tvd3a1IJBKzAQBSX8IDVFRUpM2bN2vXrl3auHGjmpqadOedd6qjo6PX/auqqhQIBKJbXl5eokcCAPRDPuecS+YLnD59WuPHj9fzzz+vhx566JLnu7u71d3dHf04EokQIQBIAeFwWBkZGZd9PunvDhg1apQmTZqkhoaGXp/3+/3y+/3JHgMA0M8k/d8BnTlzRo2NjcrNzU32SwEABpCEB+ixxx5TbW2t/vWvf+nPf/6zFixYoKFDh+r+++9P9EsBAAawhH8L7sSJE7r//vvV3t6uMWPG6I477tD+/fs1ZsyYRL8UAGAAS/qbELyKRCIKBALWYwCD2rhx4zyvqaur87ymtLTU85qjR496XgMbV3sTAveCAwCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMJP0X0gHXatKkSZ7XdHV1xfVazc3Nca1LNRs3bvS85ty5c57XdHR0eF6D1MEVEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAExwN2z0qQULFnhes2XLFs9r1q5d63mNJL3wwgtxreuvbr/99rjWlZSUeF7z7LPPel7zySefeF6D1MEVEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABggpuRok89+OCDntds377d85pUu6lovObPnx/XumHDvH9peOutt+J6LQxeXAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GSn61MyZMz2v+cMf/pCESQaHUCgU1zqfz5fgSYBLcQUEADBBgAAAJjwHaN++fZo7d65CoZB8Pt8lv6vFOac1a9YoNzdXI0eOVElJiY4dO5aoeQEAKcJzgDo7O1VYWKgNGzb0+vy6dev04osv6qWXXtKBAwd0/fXXq7S0VF1dXdc8LAAgdXh+E0J5ebnKy8t7fc45p/Xr1+vJJ5/UvHnzJEkvv/yycnJytH37dt13333XNi0AIGUk9GdATU1Nam1tVUlJSfSxQCCgoqIi1dXV9bqmu7tbkUgkZgMApL6EBqi1tVWSlJOTE/N4Tk5O9Lkvq6qqUiAQiG55eXmJHAkA0E+ZvwuusrJS4XA4uh0/ftx6JABAH0hogILBoCSpra0t5vG2trboc1/m9/uVkZERswEAUl9CA5Sfn69gMKjq6uroY5FIRAcOHFBxcXEiXwoAMMB5fhfcmTNn1NDQEP24qalJhw8fVmZmpsaNG6dVq1bpV7/6lW6++Wbl5+frqaeeUigU0vz58xM5NwBggPMcoIMHD+ruu++Ofrx69WpJ0uLFi7V582Y9/vjj6uzs1LJly3T69Gndcccd2rVrl0aMGJG4qQEAA57POeesh/iiSCSiQCBgPQa+gnhudPmXv/zF85rf/e53ntc88cQTntekos7OzrjW9fT0eF4zffp0z2v+8Y9/eF6DgSMcDl/x5/rm74IDAAxOBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMOH51zEAn2tpafG85j//+Y/nNdddd53nNX6/3/MaSeru7o5rXX81cuTIuNYdPnzY85rGxkbPa+L5e0q1v6PBjCsgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAENyNFn/rjH//oec2jjz7qeU12drbnNZJUWVnpec0///nPuF6rPysoKPC8Zs+ePZ7X/PrXv/a8Zvfu3Z7XoH/iCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHNSNGnqqqqPK+ZOHGi5zU/+MEPPK+RpB/+8Iee17zxxhue13R1dXleE88NQn0+n+c1kpSRkeF5zZ/+9CfPa7ix6ODGFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYMLnnHPWQ3xRJBJRIBCwHgP9yNChQz2vieemopL0/e9/3/Oa/Pz8uF7Lq9GjR3teM378+Lhea968eZ7XvPvuu57X/Pe///W8BgNHOBy+4o1tuQICAJggQAAAE54DtG/fPs2dO1ehUEg+n0/bt2+PeX7JkiXy+XwxW1lZWaLmBQCkCM8B6uzsVGFhoTZs2HDZfcrKynTy5Mno9uqrr17TkACA1OP5N6KWl5ervLz8ivv4/X4Fg8G4hwIApL6k/AyopqZG2dnZuuWWW7RixQq1t7dfdt/u7m5FIpGYDQCQ+hIeoLKyMr388suqrq7Wc889p9raWpWXl+vChQu97l9VVaVAIBDd8vLyEj0SAKAf8vwtuKu57777on+eOnWqpk2bpgkTJqimpkazZ8++ZP/KykqtXr06+nEkEiFCADAIJP1t2AUFBcrKylJDQ0Ovz/v9fmVkZMRsAIDUl/QAnThxQu3t7crNzU32SwEABhDP34I7c+ZMzNVMU1OTDh8+rMzMTGVmZuqZZ57RokWLFAwG1djYqMcff1wTJ05UaWlpQgcHAAxsngN08OBB3X333dGPP//5zeLFi7Vx40YdOXJEW7Zs0enTpxUKhTRnzhz98pe/lN/vT9zUAIABj5uRAgPEj370I89rtmzZEtdrxfMt87a2trheC6mLm5ECAPolAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmEj4r+QGkBwFBQXWIwAJxRUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCm5ECBvx+v+c1c+fO9bzm6NGjntdIUiQSiWsd4AVXQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACW5GChgYPXq05zXf/OY3Pa957rnnPK+RpM8++yyudYAXXAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GSlg4Hvf+16fvM5bb73VJ68DxIMrIACACQIEADDhKUBVVVW67bbblJ6eruzsbM2fP1/19fUx+3R1damiokI33nijbrjhBi1atEhtbW0JHRoAMPB5ClBtba0qKiq0f/9+7d69W+fPn9ecOXPU2dkZ3eeRRx7RO++8ozfffFO1tbVqaWnRwoULEz44AGBg8/QmhF27dsV8vHnzZmVnZ+vQoUOaNWuWwuGwfv/732vr1q367ne/K0natGmTvv71r2v//v26/fbbEzc5AGBAu6afAYXDYUlSZmamJOnQoUM6f/68SkpKovtMnjxZ48aNU11dXa+fo7u7W5FIJGYDAKS+uAPU09OjVatWaebMmZoyZYokqbW1VWlpaRo1alTMvjk5OWptbe3181RVVSkQCES3vLy8eEcCAAwgcQeooqJCR48e1WuvvXZNA1RWViocDke348ePX9PnAwAMDHH9Q9SVK1dq586d2rdvn8aOHRt9PBgM6ty5czp9+nTMVVBbW5uCwWCvn8vv98vv98czBgBgAPN0BeSc08qVK7Vt2zbt3btX+fn5Mc9Pnz5dw4cPV3V1dfSx+vp6NTc3q7i4ODETAwBSgqcroIqKCm3dulU7duxQenp69Oc6gUBAI0eOVCAQ0EMPPaTVq1crMzNTGRkZevjhh1VcXMw74AAAMTwFaOPGjZKku+66K+bxTZs2acmSJZKkF154QUOGDNGiRYvU3d2t0tJS/fa3v03IsACA1OFzzjnrIb4oEokoEAhYjwEk1YYNGzyvWb58uec1Q4cO9bwGSJRwOKyMjIzLPs+94AAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGAirt+ICuB/CgsLPa9ZsWKF5zUffPCB5zVAf8YVEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABggpuRAtdo9OjRntc45zyvOXjwoOc1QH/GFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIKbkQLX6J577vG8pr293fOaNWvWeF4D9GdcAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgZKWCgvr7e85qOjo4kTALY4QoIAGCCAAEATHgKUFVVlW677Talp6crOztb8+fPv+RbCXfddZd8Pl/Mtnz58oQODQAY+DwFqLa2VhUVFdq/f792796t8+fPa86cOers7IzZb+nSpTp58mR0W7duXUKHBgAMfJ7ehLBr166Yjzdv3qzs7GwdOnRIs2bNij5+3XXXKRgMJmZCAEBKuqafAYXDYUlSZmZmzOOvvPKKsrKyNGXKFFVWVurs2bOX/Rzd3d2KRCIxGwAg9cX9Nuyenh6tWrVKM2fO1JQpU6KPP/DAAxo/frxCoZCOHDmiJ554QvX19Xr77bd7/TxVVVV65pln4h0DADBA+ZxzLp6FK1as0Lvvvqv3339fY8eOvex+e/fu1ezZs9XQ0KAJEyZc8nx3d7e6u7ujH0ciEeXl5cUzEmAinp9xFhcXe15z5513el4DWAqHw8rIyLjs83FdAa1cuVI7d+7Uvn37rhgfSSoqKpKkywbI7/fL7/fHMwYAYADzFCDnnB5++GFt27ZNNTU1ys/Pv+qaw4cPS5Jyc3PjGhAAkJo8BaiiokJbt27Vjh07lJ6ertbWVklSIBDQyJEj1djYqK1bt+qee+7RjTfeqCNHjuiRRx7RrFmzNG3atKT8BwAABiZPAdq4caOki//Y9Is2bdqkJUuWKC0tTXv27NH69evV2dmpvLw8LVq0SE8++WTCBgYApAbP34K7kry8PNXW1l7TQACAwSHud8ElSyQSUSAQsB4DAHCNrvYuOG5GCgAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgIl+FyDnnPUIAIAEuNrX834XoI6ODusRAAAJcLWv5z7Xzy45enp61NLSovT0dPl8vpjnIpGI8vLydPz4cWVkZBhNaI/jcBHH4SKOw0Uch4v6w3Fwzqmjo0OhUEhDhlz+OmdYH870lQwZMkRjx4694j4ZGRmD+gT7HMfhIo7DRRyHizgOF1kfh0AgcNV9+t234AAAgwMBAgCYGFAB8vv9Wrt2rfx+v/UopjgOF3EcLuI4XMRxuGggHYd+9yYEAMDgMKCugAAAqYMAAQBMECAAgAkCBAAwMWACtGHDBt10000aMWKEioqK9OGHH1qP1Oeefvpp+Xy+mG3y5MnWYyXdvn37NHfuXIVCIfl8Pm3fvj3meeec1qxZo9zcXI0cOVIlJSU6duyYzbBJdLXjsGTJkkvOj7KyMpthk6Sqqkq33Xab0tPTlZ2drfnz56u+vj5mn66uLlVUVOjGG2/UDTfcoEWLFqmtrc1o4uT4KsfhrrvuuuR8WL58udHEvRsQAXr99de1evVqrV27Vh999JEKCwtVWlqqU6dOWY/W52699VadPHkyur3//vvWIyVdZ2enCgsLtWHDhl6fX7dunV588UW99NJLOnDggK6//nqVlpaqq6urjydNrqsdB0kqKyuLOT9effXVPpww+Wpra1VRUaH9+/dr9+7dOn/+vObMmaPOzs7oPo888ojeeecdvfnmm6qtrVVLS4sWLlxoOHXifZXjIElLly6NOR/WrVtnNPFluAFgxowZrqKiIvrxhQsXXCgUclVVVYZT9b21a9e6wsJC6zFMSXLbtm2LftzT0+OCwaD7v//7v+hjp0+fdn6/37366qsGE/aNLx8H55xbvHixmzdvnsk8Vk6dOuUkudraWufcxb/74cOHuzfffDO6z9/+9jcnydXV1VmNmXRfPg7OOfed73zH/fSnP7Ub6ivo91dA586d06FDh1RSUhJ9bMiQISopKVFdXZ3hZDaOHTumUCikgoICPfjgg2pubrYeyVRTU5NaW1tjzo9AIKCioqJBeX7U1NQoOztbt9xyi1asWKH29nbrkZIqHA5LkjIzMyVJhw4d0vnz52POh8mTJ2vcuHEpfT58+Th87pVXXlFWVpamTJmiyspKnT171mK8y+p3NyP9sk8//VQXLlxQTk5OzOM5OTn6+9//bjSVjaKiIm3evFm33HKLTp48qWeeeUZ33nmnjh49qvT0dOvxTLS2tkpSr+fH588NFmVlZVq4cKHy8/PV2Nion//85yovL1ddXZ2GDh1qPV7C9fT0aNWqVZo5c6amTJki6eL5kJaWplGjRsXsm8rnQ2/HQZIeeOABjR8/XqFQSEeOHNETTzyh+vp6vf3224bTxur3AcL/lJeXR/88bdo0FRUVafz48XrjjTf00EMPGU6G/uC+++6L/nnq1KmaNm2aJkyYoJqaGs2ePdtwsuSoqKjQ0aNHB8XPQa/kcsdh2bJl0T9PnTpVubm5mj17thobGzVhwoS+HrNX/f5bcFlZWRo6dOgl72Jpa2tTMBg0mqp/GDVqlCZNmqSGhgbrUcx8fg5wflyqoKBAWVlZKXl+rFy5Ujt37tR7770X8+tbgsGgzp07p9OnT8fsn6rnw+WOQ2+KiookqV+dD/0+QGlpaZo+fbqqq6ujj/X09Ki6ulrFxcWGk9k7c+aMGhsblZubaz2Kmfz8fAWDwZjzIxKJ6MCBA4P+/Dhx4oTa29tT6vxwzmnlypXatm2b9u7dq/z8/Jjnp0+fruHDh8ecD/X19Wpubk6p8+Fqx6E3hw8flqT+dT5Yvwviq3jttdec3+93mzdvdn/961/dsmXL3KhRo1xra6v1aH3q0UcfdTU1Na6pqcl98MEHrqSkxGVlZblTp05Zj5ZUHR0d7uOPP3Yff/yxk+Sef/559/HHH7tPPvnEOefcs88+60aNGuV27Njhjhw54ubNm+fy8/PdZ599Zjx5Yl3pOHR0dLjHHnvM1dXVuaamJrdnzx73rW99y918882uq6vLevSEWbFihQsEAq6mpsadPHkyup09eza6z/Lly924cePc3r173cGDB11xcbErLi42nDrxrnYcGhoa3C9+8Qt38OBB19TU5Hbs2OEKCgrcrFmzjCePNSAC5Jxzv/nNb9y4ceNcWlqamzFjhtu/f7/1SH3u3nvvdbm5uS4tLc197Wtfc/fee69raGiwHivp3nvvPSfpkm3x4sXOuYtvxX7qqadcTk6O8/v9bvbs2a6+vt526CS40nE4e/asmzNnjhszZowbPny4Gz9+vFu6dGnK/U9ab//9ktymTZui+3z22WfuJz/5iRs9erS77rrr3IIFC9zJkyfthk6Cqx2H5uZmN2vWLJeZmen8fr+bOHGi+9nPfubC4bDt4F/Cr2MAAJjo9z8DAgCkJgIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADAxP8Dpc6ClRpyf6UAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "image, label = database[26]\n",
    "plt.imshow(image, cmap = 'gray')\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2510, 0.7490, 0.2745,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.2667, 0.9529, 0.9922, 0.9765,\n",
      "          0.2471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.1176, 0.8745, 0.9922, 0.9922, 0.9686,\n",
      "          0.1608, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.2863, 0.9333, 0.9922, 0.9922, 0.9922, 0.9490,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.2863, 0.9255, 0.9922, 0.9922, 0.9922, 0.9922, 0.9490,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.7137, 0.9922, 0.9922, 0.7490, 0.9686, 0.9922, 0.5843,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.5529, 0.9922, 0.5608, 0.3373, 0.9765, 0.9922, 0.4784,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0353, 0.1412, 0.0275, 0.0549, 0.9137, 0.9922, 0.4784,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9020, 0.9922, 0.4784,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9020, 0.9922, 0.4784,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9059, 1.0000, 0.4824,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9020, 0.9922, 0.2039,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.2392, 0.9608, 0.9922, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.3843, 0.9922, 0.9922, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.1373, 0.0471, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.3843, 0.9922, 0.9922, 0.0000,\n",
      "          0.0000, 0.0353, 0.5569, 0.9137, 0.5725, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.7451, 0.9922, 0.9922, 0.5020,\n",
      "          0.0275, 0.3882, 0.9922, 0.9922, 0.7059, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.1137, 0.9020, 0.9922, 0.9922, 0.9882,\n",
      "          0.8235, 0.9922, 0.9922, 0.9922, 0.5490, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.1098, 0.8118, 0.9922, 0.9922, 0.9922, 0.9961,\n",
      "          0.9922, 0.9922, 0.9216, 0.2745, 0.0353, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.4941, 0.9922, 0.9922, 0.9922, 0.9922, 0.9961,\n",
      "          0.9922, 0.6588, 0.0745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.3098, 0.9922, 0.9922, 0.7882, 0.7451, 0.5176,\n",
      "          0.2471, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]]), 1)\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "print(train_data[70])\n",
    "print(train_data[70][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a generate custom dataset function\n",
    "+ For each data in the database convert, use one-hot encoding from pytorch to encode your label to a vector. This is necessary for computing the amount of loss.\n",
    "+ It is common to use batches of data to train our model simultaneously on a batch. The length of a batch at most is `BATCH_SIZE`.\n",
    "    - Note: you can also train your model on each data (one by one) but the training part will take too much time. \n",
    "+ Return batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 15\n",
    "INPUT_DIM = 28\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomeDataset(data, batch_size = BATCH_SIZE):\n",
    "    \"\"\" \n",
    "        Generate batches of data. Use one-hot encoding to encode your labels according to number of classes.\n",
    "        \n",
    "        Parameters:\n",
    "            - data: MNIST database\n",
    "            - batch_size = BATCH_SIZE\n",
    "\n",
    "        Returns:\n",
    "            - batches = A list of [(image_tensor, encoded label)]. Each batch's length is at most BATCH_SIZE\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    \n",
    "    # TODO\n",
    "    x = None\n",
    "    y = None\n",
    "    for image_tensor, label in data:\n",
    "        encoded_label = F.one_hot(torch.tensor(label), num_classes = NUM_CLASSES)\n",
    "        encoded_label = torch.tensor(encoded_label, dtype=torch.float32).unsqueeze(dim = 0)\n",
    "        if x is None:\n",
    "            x = image_tensor\n",
    "            y = encoded_label\n",
    "        elif len(x) < batch_size:\n",
    "            x = torch.concat((x, image_tensor), dim = 0)\n",
    "            y = torch.concat((y, encoded_label), dim = 0)\n",
    "        else:\n",
    "            batches.append((x, y))\n",
    "            x = None\n",
    "            y = None\n",
    "    batches.append((x, y))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "write your training loop.\n",
    "+ For loss function: use `CrossEntropyLoss` function\n",
    "+ Use adam optimizer\n",
    "+ To optimize your training process, you can use `optim.lr_schduler`. (Why this will improve the training process?)\n",
    "    - `your answer`\n",
    "+ Note: Also, in each time step give one row of image tensor to the model because your model is sequential (you also can use each column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/231t9s654db24dw3fmq2bn4w0000gn/T/ipykernel_1458/1925120214.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encoded_label = torch.tensor(encoded_label, dtype=torch.float32).unsqueeze(dim = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.696690767491836\n",
      "Epoch 2, Loss: 0.9307167171920318\n",
      "Epoch 3, Loss: 0.7645647101775771\n",
      "Epoch 4, Loss: 0.6570005429660813\n",
      "Epoch 5, Loss: 0.5690371615436456\n",
      "Epoch 6, Loss: 0.49447377317069424\n",
      "Epoch 7, Loss: 0.4300700551130741\n",
      "Epoch 8, Loss: 0.3748037531780071\n",
      "Epoch 9, Loss: 0.33478220478659537\n",
      "Epoch 10, Loss: 0.3048188648958063\n",
      "Epoch 11, Loss: 0.2804979829701231\n",
      "Epoch 12, Loss: 0.2600483837349604\n",
      "Epoch 13, Loss: 0.24269117399762372\n",
      "Epoch 14, Loss: 0.22800932041181515\n",
      "Epoch 15, Loss: 0.21541096133383814\n",
      "Epoch 16, Loss: 0.2040974917250002\n",
      "Epoch 17, Loss: 0.19430463522864513\n",
      "Epoch 18, Loss: 0.18527836562034397\n",
      "Epoch 19, Loss: 0.1773249728509221\n",
      "Epoch 20, Loss: 0.16986259106994314\n",
      "Epoch 21, Loss: 0.16304046559135546\n",
      "Epoch 22, Loss: 0.1566499202620126\n",
      "Epoch 23, Loss: 0.15074346987370704\n",
      "Epoch 24, Loss: 0.1454188975249717\n",
      "Epoch 25, Loss: 0.1405162517832744\n",
      "Epoch 26, Loss: 0.13606172271248737\n",
      "Epoch 27, Loss: 0.1320154333742967\n",
      "Epoch 28, Loss: 0.1283142595080251\n",
      "Epoch 29, Loss: 0.12484005186107218\n",
      "Epoch 30, Loss: 0.12161962302049918\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# create batches\n",
    "batches = CustomeDataset(train_data)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = GRU_cell(INPUT_DIM, HIDDEN_DIM, NUM_CLASSES)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# use scheduler to set learning rate\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y in batches:\n",
    "        optimizer.zero_grad()\n",
    "        h = torch.zeros(len(y), HIDDEN_DIM)\n",
    "        for i in range(INPUT_DIM):\n",
    "            h = model(x[:, i, :], h)\n",
    "            \n",
    "        pred = model(None, h, end = True)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step(total_loss/len(batches))\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(batches)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model on Test data\n",
    "Use the test data to evaluate your model.\n",
    "+ Print the accuracy of your model (should be greater than $94\\%$).\n",
    "+ Show some predictions with actual value from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/231t9s654db24dw3fmq2bn4w0000gn/T/ipykernel_1458/1925120214.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encoded_label = torch.tensor(encoded_label, dtype=torch.float32).unsqueeze(dim = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9526)\n",
      "10000\n",
      "tensor(95.2600)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # compute the accuracy of your model\n",
    "    # TODO\n",
    "    count = 0\n",
    "    dataset = CustomeDataset(test_data)\n",
    "\n",
    "    for x, y in dataset:\n",
    "        h = torch.zeros(len(y), HIDDEN_DIM)\n",
    "        \n",
    "        for i in range(INPUT_DIM):\n",
    "            h = model(x[:, i, :], h)\n",
    "\n",
    "        output = model(None, h, end = True)\n",
    "        pred = torch.argmax(output, dim = 1)\n",
    "        actual = torch.argmax(y, dim = 1)\n",
    "        count += torch.sum(pred == actual)\n",
    "        \n",
    "    print((count / len(test_data)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images of 50 predictions with actual values "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
